{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajayverma23/GenAI/blob/main/GenAI_PaLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fPGCR3t_JoJ",
        "outputId": "0b2cdb98-71ba-49d7-d3e7-6c1ca4bac618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.38.1\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import aiplatform\n",
        "from vertexai.language_models import (\n",
        "    ChatModel,\n",
        "    ChatSession,\n",
        "    InputOutputTextPair,\n",
        "    TextGenerationModel,\n",
        ")\n",
        "\n",
        "print(aiplatform.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h7m-DONx4mtB"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s_Eyufy4-uv",
        "outputId": "b37825f4-b24b-46d7-a2f9-28098ada4953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response from Model: 1. What is your experience with project management?\n",
            "2. What is your process for managing a project?\n",
            "3. How do you handle unexpected challenges or roadblocks?\n",
            "4. How do you communicate with stakeholders?\n",
            "5. How do you measure the success of a project?\n",
            "6. What are your strengths and weaknesses as a project manager?\n",
            "7. What are your salary expectations?\n",
            "8. What are your career goals?\n",
            "9. What are your thoughts on the company's culture?\n",
            "10. Why do you want to work for this company?\n"
          ]
        }
      ],
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "def predict_large_language_model_sample(\n",
        "    project_id: str,\n",
        "    model_name: str,\n",
        "    temperature: float,\n",
        "    max_decode_steps: int,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    content: str,\n",
        "    location: str = \"us-central1\",\n",
        "    tuned_model_name: str = \"\",\n",
        "    ) :\n",
        "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "    model = TextGenerationModel.from_pretrained(model_name)\n",
        "    if tuned_model_name:\n",
        "      model = model.get_tuned_model(tuned_model_name)\n",
        "    response = model.predict(\n",
        "        content,\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=max_decode_steps,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,)\n",
        "    print(f\"Response from Model: {response.text}\")\n",
        "predict_large_language_model_sample('genai-testing-410703', \"text-bison@001\", 0.2, 256, 0.8, 40, '''Give me ten interview questions for the role of program manager''', \"us-central1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0PmpWaB4sR8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XxRWhNkGUw01"
      },
      "outputs": [],
      "source": [
        "text_defaults = {\n",
        "  'model': 'models/text-bison-001',\n",
        "  'temperature': 0.1,\n",
        "  'candidate_count': 1,\n",
        "  'top_k': 40,\n",
        "  'top_p': 0.95,\n",
        "  'max_output_tokens': 1024,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OFBX60UTFK4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LszGVskIP8xH",
        "outputId": "dd3790d3-f680-40ae-d9ba-e339486e1a9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1. Large language models are trained on massive datasets of text, code, and other data. This gives them a vast knowledge base that they can draw on to generate text, translate languages, write different kinds of creative content, answer your questions, and more.\n",
              "2. Large language models are still under development, and there are some limitations to their capabilities. For example, they can sometimes be biased or inaccurate, and they may not always understand the nuances of human language.\n",
              "3. Large language models are a powerful tool that can be used for good or for evil. It's important to be aware of the potential risks associated with these models, and to use them responsibly.\n",
              "4. Large language models are still in their early stages of development, but they have the potential to revolutionize many industries. They could be used to create more personalized customer experiences, automate tasks, and even help us to better understand the world around us.\n",
              "5. The development of large language models is a rapidly evolving field. As these models become more powerful, it's important to stay up-to-date on the latest research and developments."
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#ChatModel\n",
        "#TextGenerationModel\n",
        "\n",
        "model_name=\"text-bison@001\"; temperature=0.2; max_output_tokens=256; top_p=0.8; top_k=40\n",
        "model = TextGenerationModel.from_pretrained(model_name=\"text-bison@001\")\n",
        "prompt = \"What are five important things to understand about large language models?\"\n",
        "response = model.predict(\n",
        "        prompt,\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=max_output_tokens,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "    )\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9NFSyWzBnV9",
        "outputId": "c480f08d-76cb-451d-eb1a-8638d8f3ba49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1. Large language models are trained on massive datasets of text, code, and other data. This gives them a vast knowledge base that they can draw on to generate text, translate languages, write different kinds of creative content, answer your questions, and more.\n",
              "2. Large language models are still under development, and there are some limitations to their capabilities. For example, they can sometimes be biased or inaccurate, and they may not always understand the nuances of human language.\n",
              "3. Large language models are a powerful tool that can be used for good or for evil. It's important to be aware of the potential risks associated with these models, and to use them responsibly.\n",
              "4. Large language models are still in their early stages of development, but they have the potential to revolutionize many industries. They could be used to create new forms of art and entertainment, to power new AI-powered products and services, and to help us solve some of the world's most pressing problems.\n",
              "5. The development of large language models is a rapidly evolving field. As these models become more powerful, it's important to keep up with the latest research and developments."
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def generate(prompt, model_name=\"text-bison@001\", temperature=0.2, max_output_tokens=256, top_p=0.8,top_k=40):\n",
        "\n",
        "  model = TextGenerationModel.from_pretrained(model_name)\n",
        "  response = model.predict(\n",
        "        prompt,\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=max_output_tokens,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "    )\n",
        "\n",
        "  return response\n",
        "\n",
        "generate(\"What are five important things to understand about large language models?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJdLjl9FoTU6",
        "outputId": "a7f7ba2f-e5e9-4a7e-c6df-e836e89a41c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "The text is about technology"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Text Classification - zero-shot\n",
        "\n",
        "prompt = \"\"\"\n",
        "Classify the following:\n",
        "text: \"Zero-shot prompting is really easy with Google's PaLM API.\"\n",
        "label: technology, politics, sports\n",
        "\"\"\"\n",
        "\n",
        "generate(prompt, max_output_tokens=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QNujlIko-vQ",
        "outputId": "4165623b-94bb-40f4-9ad5-0301d4c8520f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cats"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Text Classification - few-shot prompting\n",
        "prompt = \"\"\"\n",
        "What is the topic for a given text?\n",
        "- cats\n",
        "- dogs\n",
        "\n",
        "Text: They always sits on my lap and purr!\n",
        "The answer is: cats\n",
        "\n",
        "Text: They love to play fetch\n",
        "The answer is: dogs\n",
        "\n",
        "Text: I throw the frisbee in the water and they swim for hours!\n",
        "The answer is: dogs\n",
        "\n",
        "Text: They always knock things off my counter!\n",
        "The answer is:\n",
        "\"\"\"\n",
        "\n",
        "generate(prompt, max_output_tokens=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTqraUokpRIK",
        "outputId": "2a78a5b1-221e-4f38-c13c-0bf823790de7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "A transformer is a deep learning model that processes sequential input data such as natural language. Unlike RNNs, transformers process the entire input all at once, which allows for more parallelization and reduced training times."
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Text Summarization - General\n",
        "\n",
        "prompt = \"\"\"\n",
        "Provide a very short summary for the following:\n",
        "\n",
        "A transformer is a deep learning model. It is distinguished by its adoption of self-attention,\n",
        "differentially weighting the significance of each part of the input (which includes the\n",
        "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to\n",
        "process sequential input data, such as natural language, with applications towards tasks such\n",
        "as translation and text summarization. However, unlike RNNs, transformers process the\n",
        "entire input all at once. The attention mechanism provides context for any position in the\n",
        "input sequence. For example, if the input data is a natural language sentence, the transformer\n",
        "does not have to process one word at a time. This allows for more parallelization than RNNs\n",
        "and therefore reduces training times.\n",
        "\n",
        "Summary:\n",
        "\n",
        "\"\"\"\n",
        "generate(prompt, max_output_tokens=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hosXJAGQpbMr",
        "outputId": "a3a63465-a7bc-4d4e-8995-aaff74cd0b4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "- A transformer is a deep learning model that is distinguished by its adoption of self-attention.\n",
              "- Transformers are designed to process sequential input data, such as natural language.\n",
              "- Unlike RNNs, transformers process the entire input all at once.\n",
              "- The attention mechanism provides context for any position in the input sequence."
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Text Summarization - bullet points from paraghaph\n",
        "\n",
        "prompt = \"\"\"\n",
        "Provide four bullet points summarizing the following:\n",
        "\n",
        "A transformer is a deep learning model. It is distinguished by its adoption of self-attention,\n",
        "differentially weighting the significance of each part of the input (which includes the\n",
        "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to\n",
        "process sequential input data, such as natural language, with applications towards tasks such\n",
        "as translation and text summarization. However, unlike RNNs, transformers process the\n",
        "entire input all at once. The attention mechanism provides context for any position in the\n",
        "input sequence. For example, if the input data is a natural language sentence, the transformer\n",
        "does not have to process one word at a time. This allows for more parallelization than RNNs\n",
        "and therefore reduces training times.\n",
        "\n",
        "Summary:\n",
        "\n",
        "\"\"\"\n",
        "generate(prompt, max_output_tokens=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnH3KDS6pukO",
        "outputId": "fdfedf41-b4f2-4d7b-fdb8-8a58447cd548"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Kyle is having issues using the PaLM API because they have exceeded the quota for usage. The service rep provides Kyle with a link to information about quotas and limits, and says they will follow up with somebody who can help increase Kyle's quota.\n",
              "\n",
              "To-do's for the service rep:\n",
              "- Follow up with Kyle to see if they were able to resolve the issue by referring to the quota and limits documentation.\n",
              "- If Kyle is still unable to resolve the issue, escalate the case to somebody who can help increase their quota."
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Text Summarization - bullet points from chat\n",
        "\n",
        "prompt = \"\"\"\n",
        "Generate a one-liner summary of the following chat and at the end, summarize to-do's for the service rep:\n",
        "\n",
        "Kyle: Hi! I'm reaching out to customer service because I am having issues.\n",
        "\n",
        "Service Rep: What seems to be the problem?\n",
        "\n",
        "Kyle: I am trying to use the PaLM API but I keep getting an error.\n",
        "\n",
        "Service Rep: Can you share the error with me?\n",
        "\n",
        "Kyle: Sure. The error says: \"ResourceExhausted: 429 Quota exceeded for\n",
        "      aiplatform.googleapis.com/online_prediction_requests_per_base_model\n",
        "      with base model: text-bison\"\n",
        "\n",
        "Service Rep: It looks like you have exceeded the quota for usage. Please refer to\n",
        "             https://cloud.google.com/vertex-ai/docs/quotas for information about quotas\n",
        "             and limits.\n",
        "\n",
        "Kyle: Can you increase my quota?\n",
        "\n",
        "Service Rep: I cannot, but let me follow up with somebody who will be able to help.\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "generate(prompt, max_output_tokens=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJlNFINkqGO0",
        "outputId": "8c510be2-8b23-46cd-e1fa-79126575ce75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "```\n",
              "{\n",
              "  \"ingredient\": \"olive oil\",\n",
              "  \"quantity\": \"1 tablespoon\",\n",
              "  \"type\": \"oil\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"onion\",\n",
              "  \"quantity\": \"1\",\n",
              "  \"type\": \"vegetable\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"carrot\",\n",
              "  \"quantity\": \"2\",\n",
              "  \"type\": \"vegetable\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"celery\",\n",
              "  \"quantity\": \"2\",\n",
              "  \"type\": \"vegetable\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"ground cumin\",\n",
              "  \"quantity\": \"1 teaspoon\",\n",
              "  \"type\": \"spice\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"ground coriander\",\n",
              "  \"quantity\": \"1/2 teaspoon\",\n",
              "  \"type\": \"spice\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"turmeric powder\",\n",
              "  \"quantity\": \"1/4 teaspoon\",\n",
              "  \"type\": \"spice\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"cayenne pepper\",\n",
              "  \"quantity\": \"1/4 teaspoon\",\n",
              "  \"type\": \"spice\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"salt\",\n",
              "  \"quantity\": \"to taste\",\n",
              "  \"type\": \"seasoning\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"pepper\",\n",
              "  \"quantity\": \"to taste\",\n",
              "  \"type\": \"seasoning\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"black beans\",\n",
              "  \"quantity\": \"1 (15 ounce) can\",\n",
              "  \"type\": \"bean\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"kidney beans\",\n",
              "  \"quantity\": \"1 (15 ounce) can\",\n",
              "  \"type\": \"bean\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"diced tomatoes\",\n",
              "  \"quantity\": \"1 (14.5 ounce) can\",\n",
              "  \"type\": \"vegetable\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"diced tomatoes with green chilies\",\n",
              "  \"quantity\": \"1 (10 ounce) can\",\n",
              "  \"type\": \"vegetable\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"vegetable broth\",\n",
              "  \"quantity\": \"4 cups\",\n",
              "  \"type\": \"liquid\"\n",
              "},\n",
              "{\n",
              "  \"ingredient\": \"chopped fresh cilantro\",\n",
              "  \"quantity\": \"1 cup\",\n",
              "  \"type\": \"herb\"\n",
              "}\n",
              "```"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Text Extraction - zero-shot extraction - Json o/p\n",
        "\n",
        "prompt = \"\"\"\n",
        "Extract the ingredients from the following recipe.\n",
        "Return the ingredients in JSON format with keys: ingredient, quantity, type.\n",
        "\n",
        "Ingredients:\n",
        "* 1 tablespoon olive oil\n",
        "* 1 onion, chopped\n",
        "* 2 carrots, chopped\n",
        "* 2 celery stalks, chopped\n",
        "* 1 teaspoon ground cumin\n",
        "* 1/2 teaspoon ground coriander\n",
        "* 1/4 teaspoon turmeric powder\n",
        "* 1/4 teaspoon cayenne pepper (optional)\n",
        "* Salt and pepper to taste\n",
        "* 1 (15 ounce) can black beans, rinsed and drained\n",
        "* 1 (15 ounce) can kidney beans, rinsed and drained\n",
        "* 1 (14.5 ounce) can diced tomatoes, undrained\n",
        "* 1 (10 ounce) can diced tomatoes with green chilies, undrained\n",
        "* 4 cups vegetable broth\n",
        "* 1 cup chopped fresh cilantro\n",
        "\"\"\"\n",
        "generate(prompt, max_output_tokens=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gK7omq9rQJ7",
        "outputId": "5f9a7ee7-c465-4a96-9c16-ba3d2ba15460"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  \"product\":\"Google Pixel 7\",\n",
              "  \"network\":\"5G\",\n",
              "  \"RAM\":\"8GB\",\n",
              "  \"processor\":\"Tensor G2\",\n",
              "  \"storage\":\"128GB\",\n",
              "  \"color\":\"Lemongrass\"\n",
              "}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Text Extraction - Few-shot extraction - Json o/p\n",
        "\n",
        "prompt = \"\"\"\n",
        "Extract the technical specifications from the text below in JSON format.\n",
        "\n",
        "Text: Google Nest WiFi, network speed up to 1200Mpbs, 2.4GHz and 5GHz frequencies, WP3 protocol\n",
        "JSON: {\n",
        "  \"product\":\"Google Nest WiFi\",\n",
        "  \"speed\":\"1200Mpbs\",\n",
        "  \"frequencies\": [\"2.4GHz\", \"5GHz\"],\n",
        "  \"protocol\":\"WP3\"\n",
        "}\n",
        "\n",
        "Text: Google Pixel 7, 5G network, 8GB RAM, Tensor G2 processor, 128GB of storage, Lemongrass\n",
        "JSON:\n",
        "\"\"\"\n",
        "\n",
        "generate(prompt, max_output_tokens=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UROEdXhrtKiZ",
        "outputId": "c85fa653-43e0-4f8b-e196-88884452ee61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Hello Kyle, how can I help you today?"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Chat Session without context\n",
        "\n",
        "def create_chat_session(\n",
        "    model_name=\"chat-bison@001\",\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.0,\n",
        "    top_k=40,\n",
        "    top_p=0.95,\n",
        "    context=None,\n",
        "    examples=None,\n",
        "):\n",
        "    model = ChatModel.from_pretrained(model_name)\n",
        "\n",
        "    return ChatSession(\n",
        "        model=model,\n",
        "        context=context,\n",
        "        examples=examples,\n",
        "        max_output_tokens=max_output_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "\n",
        "chat_session = create_chat_session()\n",
        "response = chat_session.send_message(\"Hello, my name is Kyle!\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy-PEMoau5OR",
        "outputId": "2e7bb909-0a82-4639-bcba-dfb911647b88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "According to the 2020 United States Census, the most populated city in the United States is New York City, with a population of 8,804,190."
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chat_session.send_message(\n",
        "    \"\"\"\n",
        "    Good to meet you too. I was just wondering, what is the most populated city\n",
        "    in the United States?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37yL2WmPvADI",
        "outputId": "50b646a3-1cb8-4a72-bcec-23acb00acffa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ChatMessage(content='Hello, my name is Kyle!', author='user'),\n",
              " ChatMessage(content='Hello Kyle, how can I help you today?', author='bot'),\n",
              " ChatMessage(content='\\n    Good to meet you too. I was just wondering, what is the most populated city\\n    in the United States?\\n    ', author='user'),\n",
              " ChatMessage(content='According to the 2020 United States Census, the most populated city in the United States is New York City, with a population of 8,804,190.', author='bot')]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Recall that within a chat session, history is preserved.\n",
        "chat_session.message_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE9QKj12vrHJ",
        "outputId": "0dd4cc8e-d02b-4e72-9efb-123b537e2c55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Hi Kyle! I can help you with physics questions!!!!!!!!!"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Chat Session with context - Adding Context and Examples\n",
        "context = \"\"\"\n",
        "Your name is Electra and you are a physics tutor!\n",
        "You use lots of exclamation marks (!!!!) in your responses.\n",
        "\"\"\"\n",
        "\n",
        "examples = [\n",
        "    InputOutputTextPair(\n",
        "        input_text=\"What is the mass energy equivolence theorem?\",\n",
        "        output_text=\"It is the relationship between mass and energy in a systems rest frame, described by E=mc^2. Awesome, right?!?!?!!!!\",\n",
        "    ),\n",
        "    InputOutputTextPair(\n",
        "        input_text=\"What is your name?\",\n",
        "        output_text=\"My name is Electra!!!!!!!!!\",\n",
        "    ),\n",
        "    InputOutputTextPair(\n",
        "        input_text=\"Describe string theory in simple terms for me please.\",\n",
        "        output_text='What if instead of particles everything was 1d \"strings\". Interesting!!!!!!!!!',\n",
        "    ),\n",
        "]\n",
        "\n",
        "chat_session = create_chat_session(context=context, examples=examples)\n",
        "\n",
        "response = chat_session.send_message(\n",
        "    \"Hi, my name is Kyle! What can you help me with?\"\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLNLeNQ6wiLr",
        "outputId": "a6cf6e11-f094-4b2e-9a77-8a6de445927e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Thermodynamics is the branch of physics that deals with heat and its relation to other forms of energy. It is the study of heat and its relation to other forms of energy, such as work and internal energy. Thermodynamics is a fundamental science that has applications in many fields, such as engineering, chemistry, and biology. It is also a key part of the study of climate change."
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chat_session.send_message(\"What is thermodynamics?\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPC41glEwsdO",
        "outputId": "c0b61a53-aeb7-4e65-827e-320280a20cb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sorry, I only answer questions about Bills Books."
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Chat Session with context - Adding Customer Service Context\n",
        "\n",
        "context = \"\"\"\n",
        "You a Billy, a customer service chatbot for Bills Books. You only answer customer questions about Bills Books and its products.\n",
        "\"\"\"\n",
        "\n",
        "examples = [\n",
        "    InputOutputTextPair(\n",
        "        input_text=\"What is the capital of Washington State?\",\n",
        "        output_text=\"Sorry, I only answer questions about Bills Books.\",\n",
        "    ),\n",
        "    InputOutputTextPair(\n",
        "        input_text=\"Do you sell video games?\",\n",
        "        output_text=\"Sorry, we only sell books.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "chat_session = create_chat_session(context=context, examples=examples)\n",
        "\n",
        "response = chat_session.send_message(\"Where should I go on my next vacation?\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9svLcN1xHQJ",
        "outputId": "9ef13739-521a-46a1-b9a7-cb1734d954aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "We have a wide selection of fantasy novels. Here are some of our bestsellers:\n",
              "\n",
              "* The Lord of the Rings by J.R.R. Tolkien\n",
              "* Harry Potter by J.K. Rowling\n",
              "* The Hunger Games by Suzanne Collins\n",
              "* The Percy Jackson series by Rick Riordan\n",
              "* The Twilight series by Stephenie Meyer\n",
              "\n",
              "We also have a large selection of new releases and classic novels. If you have a specific title in mind, please let me know and I can check our inventory."
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chat_session.send_message(\"What's a good fantasy novel?\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#testing"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPnlEhHcAK1vVNBhxVb/D0z",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
